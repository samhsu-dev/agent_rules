---
description: apply only when modifying any files about testing
alwaysApply: false
---
# Testing Standards

## Test Organization
### Test Structure
- Place tests under `tests/` with clear, predictable structure
- Keep unit tests in `tests/unit/`; mirror source layout by module/component
- Name files `test_<component>.py`; group closely related cases in `Test<ComponentName>`
- Each test should assert one specific behavior

### Test Categories
#### Unit Tests
- Fast; isolated from external systems; focused on a single unit
- Live under `tests/unit/<module>/test_<component>.py`
- Test individual functions, methods, or classes
- Use mocks for external dependencies

#### Integration Tests (optional)
- Validate interaction across modules or with controlled external services
- Use lightweight fakes/mocks; run separately from unit tests when possible
- Test component interactions and data flow
- Verify system behavior under realistic conditions

## Test Structure
### Arrange-Act-Assert Pattern
- Follow Arrange-Act-Assert pattern consistently
- Use descriptive test method names: `test_<component>_<condition>_<expected>`
- Keep tests focused and isolated
- Use fixtures for common setup and teardown
- Avoid test interdependencies

## Test Naming Conventions
### Test Classes and Methods
- Test classes: `Test<ComponentName>`
- Test methods: `test_<component>_<condition>_<expected>`
- Fixtures: `<component>_<state>()` (e.g., `sampleData()`, `emptyData()`)

## Test Data Management
### Fixtures and Test Data
- Use fixtures for common test data
- Create specific fixtures for different scenarios
- Use temporary files for file operations
- Clean up resources automatically
- Use factories for complex object creation

## Assertion Guidelines
### Assertion Best Practices
- One assertion per test when possible
- Use specific assertion methods
- Test both positive and negative cases
- Verify error conditions explicitly
- Use descriptive assertion messages

## Test Documentation
### Documentation Standards
- Write clear test descriptions
- Explain complex test scenarios
- Document test data requirements
- Keep test code readable and maintainable
- Use comments to explain test intent

## Test Configuration
### Test Framework Setup
- Use appropriate testing framework for the language
- Configure test discovery appropriately
- Use markers for test categorization
- Enable strict mode for better error reporting
- Set up test environment consistently

### Test Running
- Use appropriate test runner
- Add verbose output when needed
- Stop on first failure when debugging
- Use concise tracebacks for better readability
- Run tests in parallel when possible

### Test Coverage
- Aim for high test coverage (>90%)
- Focus on critical paths and edge cases
- Test error conditions and boundary values
- Verify exception handling
- Use coverage tools to identify gaps

## Test Maintenance
### Adding New Tests
1. Create test file in appropriate directory
2. Follow naming conventions
3. Use existing fixtures when possible
4. Write focused, isolated tests
5. Update documentation if needed

### Test Debugging
- Use descriptive test names
- Add debug prints when necessary
- Use test runner's debug flags
- Use debugging tools when needed
- Isolate failing tests for investigation

### Test Performance
- Keep tests fast and focused
- Use mocks for external dependencies
- Avoid complex setup in test methods
- Use fixtures for expensive operations
- Run tests in parallel when possible

## Mocking and Stubbing
### Mock Strategy
- Mock external dependencies
- Use dependency injection for testability
- Create test doubles for complex objects
- Verify mock interactions when needed
- Use appropriate mock types (mocks, stubs, fakes)

### Mock Best Practices
- Mock at the boundary of the system under test
- Use mocks to isolate the unit under test
- Verify important interactions
- Don't over-mock; test real behavior when possible
- Use mocks to simulate error conditions

## Test Data Patterns
### Test Data Creation
- Use builders for complex objects
- Create minimal test data
- Use realistic but simple test data
- Avoid hardcoded values in tests
- Use data factories for consistency

### Test Data Cleanup
- Clean up test data after tests
- Use transactions for database tests
- Remove temporary files
- Reset global state
- Use test isolation techniques

## Language-Specific Considerations
### Testing Frameworks
- Use appropriate testing framework for the language
- Follow language-specific testing conventions
- Use language-specific assertion libraries
- Follow language-specific mocking patterns

### Test Organization
- Follow language-specific test organization patterns
- Use appropriate test discovery mechanisms
- Follow language-specific fixture patterns
- Use language-specific test configuration

## Continuous Integration
### CI/CD Integration
- Run tests automatically on code changes
- Fail builds on test failures
- Generate test reports
- Run tests in different environments
- Use test results for quality gates

### Test Automation
- Automate test execution
- Generate test reports
- Track test metrics
- Use test results for decision making
- Integrate with deployment pipelines